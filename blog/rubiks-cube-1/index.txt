1:"$Sreact.fragment"
2:I[8308,["177","static/chunks/app/layout-0c4d063dd6fe438d.js"],"ThemeProvider"]
3:I[5244,[],""]
4:I[3866,[],""]
5:I[8173,["560","static/chunks/app/blog/%5Bslug%5D/not-found-4509230feeaeaa83.js"],""]
7:I[6213,[],"OutletBoundary"]
9:I[6213,[],"MetadataBoundary"]
b:I[6213,[],"ViewportBoundary"]
d:I[4835,[],""]
:HL["/_next/static/css/116e80e7d664f8f5.css","style"]
0:{"P":null,"b":"9C5e4BNY8XVkf_5bg9xPV","p":"","c":["","blog","rubiks-cube-1",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","rubiks-cube-1","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/116e80e7d664f8f5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"className":"dark","children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&display=swap","rel":"stylesheet"}],["$","meta",null,{"name":"msapplication-TileColor","content":"#000000"}],["$","meta",null,{"name":"theme-color","content":"#000000"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              const savedTheme = localStorage.getItem('theme') || 'dark';\n              document.documentElement.classList.add(savedTheme);\n\n              window.MathJax = {\n                tex: {\n                  inlineMath: [['\\\\(', '\\\\)']],\n                  displayMath: [['$$', '$$']],\n                  processEscapes: true,\n                },\n                options: {\n                  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n                }\n              };\n            "}}],["$","style",null,{"children":"\n          /* Mobile: Smaller base font size to counteract no scaling */\n          @media (max-width: 767px) {\n            html { font-size: 12px; }\n          }\n          /* Desktop: Normal font size with scaling */\n          @media (min-width: 768px) {\n            html { font-size: 16px; }\n          }\n        "}]]}],["$","body",null,{"className":"antialiased transition-colors duration-100 dark:bg-black dark:text-white bg-white text-black","children":["$","$L2",null,{"children":["$","div",null,{"className":"min-h-screen flex justify-center items-center","children":["$","div",null,{"className":"scale-100 md:scale-[1.0] origin-top w-full","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","rubiks-cube-1","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$0:f:0:1:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],["$","div",null,{"className":"min-h-screen flex items-center justify-center","children":["$","div",null,{"className":"text-center space-y-4","children":[["$","h1",null,{"className":"text-4xl font-bold","children":"404"}],["$","h2",null,{"className":"text-xl","children":"Post Not Found"}],["$","p",null,{"className":"text-gray-500","children":"The blog post you're looking for doesn't exist."}],["$","$L5",null,{"href":"/","className":"text-blue-500 hover:underline","children":"Return Home"}]]}]}]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",null,["$","$L7",null,{"children":"$L8"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","n2PiFoJI1RdOD0uUt_osk",{"children":[["$","$L9",null,{"children":"$La"}],["$","$Lb",null,{"children":"$Lc"}],null]}]]}],false]],"m":"$undefined","G":["$d","$undefined"],"s":false,"S":true}
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Teaching Language Models to Solve Rubik's Cubes (Part I)"}],["$","meta","2",{"name":"description","content":"How hard could it be?"}],["$","link","3",{"rel":"manifest","href":"/site.webmanifest","crossOrigin":"$undefined"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","sizes":"any"}],["$","link","5",{"rel":"icon","href":"/favicon-16x16.png","sizes":"16x16","type":"image/png"}],["$","link","6",{"rel":"icon","href":"/favicon-32x32.png","sizes":"32x32","type":"image/png"}],["$","link","7",{"rel":"icon","href":"/favicon-192x192.png","sizes":"192x192","type":"image/png"}],["$","link","8",{"rel":"icon","href":"/favicon-512x512.png","sizes":"512x512","type":"image/png"}],["$","link","9",{"rel":"apple-touch-icon","href":"/apple-touch-icon.png","sizes":"180x180"}],["$","link","10",{"rel":"mask-icon","href":"/safari-pinned-tab.svg","color":"#000000"}]]
8:null
e:I[8881,["321","static/chunks/321-14e9b0818460c0f7.js","738","static/chunks/738-132473797a2960c5.js","953","static/chunks/app/blog/%5Bslug%5D/page-727f3d52741632b8.js"],"default"]
f:I[7738,["321","static/chunks/321-14e9b0818460c0f7.js","738","static/chunks/738-132473797a2960c5.js","953","static/chunks/app/blog/%5Bslug%5D/page-727f3d52741632b8.js"],"default"]
10:T62a1,<h2>What are long horizon tasks (and why should we care about them)?</h2>
<p>Science-fiction AIs such as TARS, Cortana or the Terminator all exhibit a quality that today’s systems still lack, which is the capacity to pursue far-off goals on their own. We do have “agents”, but once the chain of actions stretches beyond a certain number of turns their performance collapses, and there&#39;s no easy way of training these models. These <em>long-horizon tasks</em> are exactly what I care about - how can we teach a language model to carry out long-horizon tasks without paying an astronomical training bill? This is exactly we will try to explore in this blog-series. In part I of this series, we will focus on using reinforcement learning methods head-on and analyse our results. In part II, we will use our learnings to make something better. </p>
<p>But first.. we need a proxy task to experiment on.</p>
<h2>Why Must We Solve a Cube?</h2>
<p>Ever solved a Rubik&#39;s cube? It&#39;s <em>deceptively</em> hard when you&#39;re starting out. Sure, you can scramble it in a few seconds, but the state space you&#39;ve just plunged into is combinatorial, and it contains roughly around \(4.3 \times 10^{19}\) possible configurations! To put it to scale, if you were to stack a tower of cubes for every state, you&#39;d be shooting way past the current brightest star in the sky (Sirius) and then do it again past planet <a href="https://www.halopedia.org/Reach">Reach</a>. That&#39;s a lot of cubes. Still, traditional algorithmic solvers struggle with this enormousness, and this is why clever methods like the <a href="https://kociemba.org/math/twophase.htm">two phase solver</a> exist—they narrow down the search space by satisfying a set of conditions. Learnt algorithms are the same way, they create latent representations and have biases which allow them to cut through most of the search space.</p>
<p>So you think about it, a Rubik&#39;s cube is exactly the kind of long-horizon problem we&#39;re looking to test - it takes multiple steps to reach the solution space, and it is verifiable that they reached a solution. The state graph is vertex-transitive (every scramble looks like every other scramble from the right vantage point), so there are no privileged &quot;easy corners&quot; where the agent can camp. Best of all, God, on an alias of the brute-force cluster that exhaustively enumerated the cube state group - has announced that the diameter of this graph is <a href="https://www.cube20.org/">20 moves in the half-turn metric</a>. Twenty! That is a <em>constant</em> that fits inside a tweet (and makes our problem tractable to learn for an agent), yet the shortest path between two arbitrary vertices is still long enough to punish greedy myopia.<br><img src="/images/godstable.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>The cube is also <em>deterministic</em>. Which means, in principle, you only need the starting state to solve the whole thing - that&#39;s why blindfolded cubing exists! But it&#39;s hard because you cannot state-action mappings, the key again is cutting down these massive search spaces through learnt approaches. Humans get around this by only memorizing small algorithms like PLL using learning and pattern recognition to know <em>when</em> to apply a combination of these algorithms to get closer to the solution.</p>
<h2>How are we going to do it?</h2>
<p>There are several ways to teach a language model a new skill. If you want it deeply integrated into the model&#39;s fundamental capabilities, you&#39;d include it heavily in the pretraining data mix. But pretraining is prohibitively expensive, and continued pretraining faces the same cost barrier. Supervised fine-tuning is cheaper - you just create input-output examples demonstrating the skill. The problem is that it&#39;s shallow. The model learns to pattern-match and mimic rather than truly internalizing the underlying capability. What if instead you could simulate an environment where the model experiments and learns through trial and error, discovering the skill on its own? That&#39;s exactly what reinforcement learning does, and unless you&#39;ve been living under a rock, you&#39;ve probably heard about it by now.</p>
<p>There&#39;s actually another reason that we&#39;re going to choose a Rubik&#39;s cube as our long-horizon problem of choice : The cube itself makes for a fascinating reinforcement learning study because it sits at this uncomfortable intersection where it <em>feels</em> like it should be tractable for modern RL, but it keeps resisting straightforward RL approaches. So that&#39;s <em>exactly</em> what we&#39;re going to do - hit the ground running with some straightforward approaches and see if they fail, and then try to investigate why.</p>
<h2>A Quick Detour: What&#39;s GRPO?</h2>
<p>Now, there&#39;s various reinforcement learning algorithms that we can utilize to train our language model how to solve a rubik&#39;s cube. Among those dozens of policy-gradient acronyms floating around, Group Relative Policy Optimization is the one whose reference implementation already speaks transformer and comes with a great support from the verifiers library. That single convenience is why we borrow its machinery for the cube.</p>
<p>I&#39;ll only cover GRPO on a high level, but here are some resources if you want to read more about the internals of how GRPO works.</p>
<p>The ritual is actually disarmingly short. For a given scramble \(s\) we prompt the network once and sample a <em>batch</em> of roll-outs:</p>
<p>$$ {\tau_1, \tau_2, \ldots, \tau_G}, \quad \text{each } \tau_i = (a_{i,1}, \ldots, a_{i,T_i}) $$</p>
<p>A <em>verifier</em> (in our case a Kociemba solver) labels every trajectory with a sparse reward</p>
<p>$$ R(\tau_i) = \mathbb{1}[\text{cube-state after } \tau_i \text{ is solved}] $$</p>
<p>GRPO turns the batch into a <em>relative</em> signal. Let:</p>
<p>$$ \bar{A}_i = R(\tau_i) - \langle R \rangle $$</p>
<p>be the advantage of trajectory \(i\) versus the batch mean \(\langle R \rangle\). The policy gradient is</p>
<p>$$ J_{\text{GRPO}}(\theta) = \mathbb{E}<em>{q, {o_i}} \left[ \frac{1}{G} \sum</em>{i=1}^{G} \min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i, \text{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right) A_i \right) - \beta D_{\text{KL}}[\pi_\theta || \pi_{\text{ref}}] \right] $$</p>
<p>In words: up-weight the moves that appear in <em>above-average</em> roll-outs, down-weight the rest. The baseline is not a learned value network but the empirical mean of the <em>same</em> group, so the update is <em>zero-centred</em> by construction and needs no critic. When the model samples a group of trajectories, the algorithm pushes probability mass toward whichever ones scored above the group average. This works beautifully when your model can already occasionally stumble into some success, because then you have clear winners to amplify. </p>
<h2>Experiments with Reward Modeling</h2>
<p>To get started, we need to first create our environment which will allow our language model to interact with a rubiks cube. And to create our environment, we need to solve two problems : 1) problem representation 2) reward model.</p>
<p>Let&#39;s quickly jump back to how humans solve Rubik&#39;s cubes ; each mental step we decide to play an algorithm of \(N\) moves. We can model after this behavior to incentivize the model to make its own algorithms ; a multi-turn setup where the model could take \(K\) turns of maximum \(N\) moves each. For verifiability, we will use the very popular <a href="https://www.speedsolving.com/wiki/index.php/Singmaster_notation">singmaster notation</a>: Each face gets a letter: F (front), B (back), U (up), D (down), L (left), R (right). The letter alone means turn that face 90° clockwise. We add a prime symbol (like R&#39;) for counterclockwise, or a 2 (like U2) for 180°. So &quot;R U R&#39; U&#39;&quot; means: right clockwise, up clockwise, right counter-clockwise, up counter-clockwise.</p>
<p><img src="/images/singmaster.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>The cube state has to be represented as text and each face (U/L/F/R/B/D) gets its own grid format with colors as single letters, looking something like <code>TOP(U): WRB/OWG/YWW</code>. It&#39;s regex-friendly, preserves spatial relationships, and reasonably compact. The model interacts through Singmaster notation by outputting moves in <code>&lt;move&gt;...&lt;/move&gt;</code> tags.</p>
<p>For the RL setup, I used GRPO with a multi-turn structure where the model could take up \(K\) turns of \(N\) moves each. Since a sparse reward may be too harsh for the our policy when we&#39;re just starting out, we shape the reward for rewarding progress and efficiency while keeping the reward function as positive only:</p>
<ul>
<li>A small format bonus of \(0.1\) for using the correct <code>&lt;move&gt;...&lt;/move&gt;</code> tags</li>
<li>If it solves the cube: base reward of \(1.0\) plus an efficiency bonus \(\min(1.0, \frac{d}{t})\) where \(d\) is the initial distance and \(t\) is turns used</li>
<li>If it doesn&#39;t solve it: a progress reward of \(\frac{\max(0, d - d_{\text{final}})}{d}\) that measures how much closer it got</li>
</ul>
<p>During designing the reward, the primary idea is to keep enough variance between good and bad approaches. Remember, GRPO setup samples multiple rollouts per prompt, groups them, and pushes probability mass toward better-performing completions <strong>within each group</strong>.</p>
<h3>Curriculum</h3>
<p>Feeding the agent 1–20-move scrambles on day one is educational malpractice. The trainer therefore exposes only scrambles in a user-chosen band \([s_{\min}, s_{\max}]\) and can be configured to expand the band once the rolling success rate exceeds a threshold. This is in contrast with an approach like DeepCubeA where they train their network on the full combination range, where there&#39;s no explicit curriculum but more of an emergent one because the model learns the combinations in the order of easy to hard. However, they train for hundreds of thousands of steps, which we cannot, so we schedule ours by hand and keep the wall-time budget honest.</p>
<h3>The Diagnostic: The 1-Move Test</h3>
<p>Before we dream of twenty-move solutions we check whether the network can walk one step. This will both act as a baseline test for the capability of our models to solve the cube because we&#39;re testing if it can execute the most basic moves.</p>
<p>A 1-move scramble has exactly twelve legal continuations (6 Faces x 2 Directions); one of them rewinds the scramble, the other eleven leave the cube mixed. This will essentially make our reward structure sparser and essentially cancels out the &quot;efficiency&quot; term if the cube is unsolved and doubles it if it&#39;s solved, but since we&#39;ve reduced our search space, it should be tractable.</p>
<p>A random policy should succeed with a probability \(\frac{1}{12} \approx 8.3%\) per attempt. So even if we have 16 rollouts per group and given a single episode step, you should hit the correct move occasionally just by chance, and GRPO should immediately amplify it. However, this is not what we see in practice!</p>
<p><img src="/images/flatreward.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>As you can see, the mean reward stays relatively flat throughout and hugs the format bonus of 0.1. This is <em>not</em> good and means that the model is generally not learning! Well, why is this happening? We can try and diagnose the issue by analyzing the completion logs to get a sense of what was going on. Using the logs we can plot the moves that the model likes to take, the moves it gets correct and the overall reward distribution.</p>
<p><img src="/images/chart.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>[<em>Note : We&#39;re only logging every 2 steps so not all the steps were captured, but this should be a decent sample size to allow us to understand what&#39;s going on.</em>]</p>
<p>So, across 1,832 attempts we get 10 successful solves. That&#39;s a 0.55% success rate, and that&#39;s worse than random by about 10x! What&#39;s interesting is that it&#39;s not that the model has collapsed to a single move even though it&#39;s heavily skewed. We do see that the model clearly explores, but it just explores wrongly. It plays R about 420 times (which is nearly a quarter of the attempts) followed by B, L, D, U in decreasing frequency, and never really any prime moves or double rotations.</p>
<p>This extreme bias essentially creates extreme reward sparsity because there&#39;s no clear pattern for GRPO to latch onto, because there&#39;s basically no in group variance in most cases. When all your rollouts never solve anything, there&#39;s nothing to amplify. This is what happens when the prior is too strong, the signal becomes too sparse and too noisy to overcome the strong prior. There&#39;s a chance that this may work when scaled up, but that&#39;s an insane amount of compute to introduce a base capability!</p>
<h2>Reward-shaping, curricula, prayers</h2>
<p>Once I realized that the 1-move test was failing, I tried getting inspired by Potential-Based Reward Shaping (PBRS)—adding \(\gamma \Phi(s&#39;) - \Phi(s)\) where \( \Phi(s) = -0.5 \cdot d(s)\). In traditional RL with learned value functions, this is theoretically sound because the value function absorbs the potential. But GRPO works on <em>relative</em> differences within rollout groups so this results in shifting every advantage by the same constant and leaves the <em>relative</em> structure untouched.</p>
<p>I also tried explicit distance feedback (telling the model &quot;you&#39;re now \(k\) moves from solved&quot; after each action), and showing the solved state as a reference ; thinking traces got more confident but they were still confidently wrong, so scores didn&#39;t improve. I tried various progress ratios, efficiency bonuses, completion bonuses. None of it mattered because the underlying problem was that the model never reached differentiated outcomes.</p>
<p>Reasoning models actually consistently outperformed non-reasoning models. I hypothesis this is not because they were reasoning better about cube mechanics, but because longer generations = more stochasticity in the action distribution = slightly more exploration. Still not enough to overcome mode collapse, though.</p>
<p><img src="/images/thinkingbetter.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>The representation experiments provided the final irony. I held a hypothesis that if we give the model a cube representation that is more represented in the dataset (by simply sampling the model about it), we may get better or more confident results. And yes! When I gave the model the flat unfolded net - the representation it found most natural - it started generating moves that looked <em>scarily</em> informed. &quot;Ah, I see the orange face needs to move to the top layer,&quot; it would declare, before proposing a sequence that would make any cuber weep. The model had learned to <em>talk</em> about cubes like an expert while remaining fundamentally unable to <em>solve</em> them. The distribution of the moves taken did not budge much but the models simply got more confident at justifying their moves!</p>
<p><img src="/images/wrongmove.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<h2>Post-mortem</h2>
<h3>What GRPO does (and what it needs)</h3>
<p>Let&#39;s be precise about how GRPO works. For each question, say \(q\), GRPO samples a group of outputs which we can represent as \({o_1, o_2, o_3, \ldots, o_G}\) from the old policy \(\pi_{\theta_{\text{old}}}\) and scores them with some rewards \({r_1, r_2, r_3, \ldots, r_G}\) and then computes the advantage function to measure how much better a particular response is compared to the average response in the group:</p>
<p>$$ A_i = \frac{r_i - \text{mean}({r_1, r_2, \ldots, r_G})}{\text{std}({r_1, r_2, \ldots, r_G})} $$</p>
<p>Verifiers by default uses a &quot;corrected&quot; GRPO variation that removes the normalization term from the advantage calculation. So the advantage actually becomes:</p>
<p>$$ A_i = r_i - \text{mean}({r_1, r_2, \ldots, r_G}) $$</p>
<p>Notice the critical thing here again, the advantage is literally coming from the <em>relative differences between the group</em>. When we talked about GRPO earlier, I briefly mention that GRPO works well if there&#39;s enough variance. However, if all the rewards in a group are similar, something else happens - the advantage \(A_i \to 0\) for all \(i\). Let&#39;s find out the downstream effects of this.</p>
<p>Now, the advantage will be used to optimize the policy by maximizing:<br>$$ J_{\text{GRPO}}(\theta) = \mathbb{E}_{q, o_i} \left[ \frac{1}{G} \sum_{i=1}^{G} \min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i, \operatorname{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right) A_i \right) - \beta D_{\text{KL}}[\pi_\theta || \pi_{\text{ref}}] \right] $$</p>
<p>If all your rewards are similar, the advantage \(A_i \to 0\) for all \(i\). If we plug this into the policy:</p>
<p>$$ J_{\text{GRPO}}(\theta) \approx \mathbb{E}_{q, o_i} \left[ \frac{1}{G} \sum_{i=1}^{G} 0 - \beta D_{\text{KL}}[\pi_\theta || \pi_{\text{ref}}] \right] = - \beta D_{\text{KL}}[\pi_\theta || \pi_{\text{ref}}] $$</p>
<p>So when \(A_i \to 0\) the objective reduces to just minimizing KL divergence from the reference policy! This effectively turns into a no-op because we&#39;re just trying to stay close to where we started and stay anchored to our initial bad policy. We can actually see this reflected in a very very small (and often zero!) grad norm which spikes from these incredibly sparse updates!</p>
<p><img src="/images/gradnorm.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>Let&#39;s calculate how many times do we actually get useful signal in our group from our 0.55% success rate. Suppose the number of rollouts per group is 16, then the probability that there&#39;s at least one success becomes:</p>
<p>$$ P(\text{at least 1 success}) = 1 - (1 - 0.0055)^{16} = 1 - (0.9945)^{16} \approx 0.084 $$</p>
<p>So only about 8.4% of groups have any success at all. In the other 91.6% of groups, all rewards cluster around 0.1 (format reward), all advantages are near zero, and the gradient collapses to pure KL regularization. This is also what the reward distribution graph justifies.</p>
<p>This immediately points towards the hypothesis that any strong baseline should immediately help in allowing a model to learn further. I am of course, remotely not the first person to observe this - if we look at the DeepSeekMath paper (which introduces GRPO), they try to explain why reinforcement learning works in the first place:</p>
<p><img src="/images/deepseeksnippet.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>Translation : They compare compares Pass@K (&quot;does <em>any</em> sample solve it?&quot;) with Maj@K (&quot;does majority voting solve it?&quot;). GRPO boosts Maj@K while barely touching Pass@K.</p>
<p>GRPO doesn&#39;t teach new skills. It shifts probability mass toward solutions the model can already sometimes produce. If your Pass@K is 0, the algorithm has nothing to amplify; the relative advantages stay zero and you burn compute turning the KL crank. Our 4B and 3B checkpoints sit squarely in the &quot;Pass@K ≈0&quot; regime.</p>
<p>There&#39;s a whole subfield which does study capability aquisition, and the consensus is quite clear: <a href="https://arxiv.org/abs/2507.10616">&quot;capability acquisition occurs during pre-training and continual fine-tuning, while GRPO mainly amplifies skills the base model already has&quot;</a> and <a href="https://huggingface.co/blog/lmassaron/gemma-grpo">&quot;The method works by leveraging existing capabilities that do not easily emerge from greedy decoding but sometimes show up when working with higher temperatures.&quot;</a></p>
<p>But does any model hold the mandate of the cube? (Or, at least for 1 move?). Well, I put OpenAI&#39;s latest family through the wringer. Here&#39;s what Pass@5 performance looks like:</p>
<p><img src="/images/gpt5cube.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<p>We can clearly see the capabilities evaporating as the parameter budget decreases and drops between some latent representation threshold. Our Qwen checkpoints sit <em>below</em> GPT-5-nano, i.e. in the region where Pass@K is indistinguishable from zero.</p>
<p>However, the GPT-5 gap just doesn&#39;t sit right, so I ran some head-to-head on 1-move scrambles across families, and reran GPT-5.</p>
<table>
<thead>
<tr>
<th>model</th>
<th>avg reward / 2.0</th>
<th>solves / 50</th>
<th>equiv. %</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-5</td>
<td>1.76</td>
<td>44</td>
<td>88 %</td>
</tr>
<tr>
<td>Claude Sonnet 4.5</td>
<td>0.60</td>
<td>15</td>
<td>30 %</td>
</tr>
<tr>
<td>Claude Opus 4</td>
<td>0.36</td>
<td>9</td>
<td>18 %</td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td>0.20</td>
<td>5</td>
<td>10 %</td>
</tr>
<tr>
<td>Kimi k2</td>
<td>0.04</td>
<td>1</td>
<td>2 %</td>
</tr>
<tr>
<td>Qwen-235B</td>
<td>0.00</td>
<td>0</td>
<td>0 %</td>
</tr>
<tr>
<td>Qwen-4B</td>
<td>0</td>
<td>0</td>
<td>0 %</td>
</tr>
<tr>
<td>our Qwen-4B</td>
<td>~0</td>
<td>1 / 183</td>
<td>0.55 %</td>
</tr>
</tbody></table>
<p>Remember, random guessing sits at 8.3%. GPT-5 seems to be playing another game entirely, and the chinese models seem to be terrible shape rotators. I did expect claude to score higher than what it did, so that indeed comes as a surprise.||||</p>
<h2>When in doubt, blame the data (and then the parameters)</h2>
<p>The corpus giveth and the corpus withholdeth. Internet text should be largely unhelpful to all the models in this regard. If we peek at the forums, they overflow with something like &quot;spam the R U R′ U′ until the corner permutes&quot; and &quot;white cross first&quot;, but almost nobody writes out the full 54-sticker before/after. The causal model &quot;R maps <em>this</em> state to <em>that</em> state&quot; is largely absent from the raw corpus, unless OpenAI has hands on some significantly large piece of multiturn puzzle data that it has generalized on.</p>
<p>Distillation tells the same story. Qwen 4B inherited every linguistic prior its 235B teacher had, hence the obsessive over-generation of R. But since the teacher itself never learned the conditional mapping, there was nothing <em>to</em> distil. Strong-to-weak knowledge transfer works only when the <em>strong</em> model actually possesses the knowledge. Compression can&#39;t create what isn&#39;t there. And our empirical observation with parameter scale dictates that the incredibly smaller distilled model should probably only perform worse.</p>
<p>To put all of this bluntly, the literature had drawn the map in fluorescent ink:</p>
<p><strong>&quot;Here be dragons - bring either a bigger model, a distilled prior, or architectural biases that match the domain.&quot;</strong></p>
<p>And so I charged in with a 4B param scribbler and a dream.</p>
<p>The dragons won... for now.</p>
<p><img src="/images/againstdragons.png" alt="alt text" title="" class="rounded-lg w-full"></p>
<h2>What&#39;s Next (Part II)?</h2>
<p>GRPO can&#39;t bootstrap capability from nothing. So we stop praying for emergent algorithms and hand them to the model instead. Our cube simulator is also a bottomless generator for SFT : scramble, ask Kociemba for the optimal reversal, record the trace, repeat. Now you may say &quot;erm, that&#39;s memorization&quot; - but memorisation is not a dirty word here because it&#39;s needed as a warmup for building our functional baseline. Once the weights have cached the macro patterns we can go back to RL to <em>compress</em> them.</p>
<p>However, we can also only do SFT for low-depths in our scramble graph since the possible state space expands exponentially as the number of moves goes up. Another hard learnt lesson for RL is to take the pre-RL baseline more seriously because some tasks may just be too hard for a model to start with. So once that supervised checkpoint reaches a respectable solve-rate on held-out 5-move scrambles we unfreeze the RL loop. The answer also may lie in simply choosing a better model with an more acceptable baseline than Qwen, but I am not giving up on my tiny warriors just yet.</p>
<p><img src="/images/cubegraph.png" alt="cubegraph" title="" class="rounded-lg w-full"><br>GRPO now starts from a prior that already respects primes and doubles, so its job is reduced to <em>compression</em> by shaving excess turns and stitching algorithms. We can further augment this by doing something like multi-token prediction, but instead predict multiple moves at once, like our original environment envisioned, and then reward based on that. If all of this does not work, we can also look into less complex cubes, like 2x2 cubes with a much smaller state-space.</p>
<p>All shall be revealed in part II.</p>
11:T4f65,<p>This is another paper in the YOLO iteration. I like vision papers a lot and these are my notes from my reading of the paper.</p>
<p>Original paper <a href="https://arxiv.org/abs/2402.13616">link</a>.</p>
<h1>What exactly is the problem with current models?</h1>
<p> Object detection models can be simplified as learning the most important features in a training set which can allow it to predict the location (bounding box) or segmentation map of a given test object. Ideally, we want such models to be robustly generalized - an object detection world model which can one shot any test object in any scenario as long as it has seen objects of a similar class. There has been a push towards achieving this ideal but not without roadblocks in convergence, which the paper focuses a lot on. </p>
<p> The authors argue that in part a lot of poor/slow convergence is caused by the model not being able to strike the perfect balance between compression (learning the least amount of important features to get the job done) and relevance (which features are actually the most relevant?). This is essentially what the information bottleneck problem is.</p>
<h1>What&#39;s an information bottleneck?</h1>
<p>An information bottleneck is when we compress high dimensional input data into lower dimension features by retaining the most important features to reduce computational complexity for training and inference, but at the same time it leads to data loss because we have to trim some features. </p>
<p>This is what almost every modern deep learning method does, and is essentially its goal. Deep learning is an inherent compression optimization problem. A model tries to learn the least amount of most important features by a priority basis - that is how we get feature maps and attention. The best model is the one which generalizes perfectly over a concept so as to require learn the least number of features possible while retaining enough information to achieve the best results.</p>
<p>It&#39;s hard to measure the ideal amount of information loss from information bottlenecks. The authors in the paper argue that the loss from information bottlenecks may still be non-negligible in many modern techniques because it is simply learning the wrong features and hence the wrong mapping between input and predictions.</p>
<p><div class="flex justify-center my-8"><img src="/images/infobottleneck1.png" alt="alt text" title="Information Bottleneck" class="rounded-lg w-1/2"></div></p>
<p>The information loss due to the bottleneck can be described by in terms of mutual information:</p>
<p>$$<br>    I(X,X) \ge I(X, f\theta(X) \ge I(X, g\phi(f\theta(X)))<br>$$</p>
<p>Here \(I\) is the mutual information. Mathematically, \(MI(X;Y)\) is defined  as:</p>
<p>$$<br>MI(X;Y) = ∑p(x,y) \log_2(\frac{p(x,y)}{p(x)p(y)})<br>$$</p>
<p>..where \(p(x)\) and \(p(y)\) are the marginal probability distributions of X and \(Y\), respectively, and \(p(x,y)\) is their joint probability distribution. Intuitively, MI measures how much knowing the value of X reduces uncertainty about Y, or conversely, how much knowing the value of Y reduces uncertainty about X. \(f\) and \(g\) are transformation functions with trainable parameters \(\theta\) and \(\phi\).</p>
<p><strong>What does this even mean?</strong></p>
<p><div class="flex justify-center my-8"><img src="/images/infobottleneck2.png" alt="alt text" title="Information Bottleneck" class="rounded-lg w-1/2"></div></p>
<ol>
<li><p>This represents that as more neural transformations are applied, the more information is lost. As in, deeper layers mean more information loss since there&#39;s consecutive application of transformation functions (neurons).</p>
</li>
<li><p>This means a model with deeper layers retain lesser info about both the input and target.<br>Hence it would naturally perform worse.</p>
</li>
<li><p>A model with larger number of parameters has much more parameters and can learn larger number of features (information) about the data.<br>This is why width is important in deep networks than depth itself.</p>
</li>
<li><p>This increase in width can only increase the scope of learning more information by simply increasing the number of params, but the information loss per param is still the same (or, often increased because of more connections).</p>
</li>
</ol>
<h1>How do we get rid of this data loss?</h1>
<p>The paper identifies 3 ways of dealing with data loss from information bottleneck.</p>
<ol>
<li><p><strong>Reversibility</strong>: Reversibility is a method where we can compute/reconstruct the activations of a hidden/intermediate layer \(Y_N\) from a subsequent layer \(Y_{N+1}\) during backpropagation. By eliminating the need to store intermediate activations, this approach significantly enhances memory efficiency, leading to more compact networks. However, reversible architectures often require additional layers, increasing computational complexity as a trade-off for reduced memory usage.</p>
</li>
<li><p><strong>Masked modeling</strong>: Masked modeling improves feature extraction by training the model to predict missing (masked) features in the input data, using a reconstruction loss. This loss measures how effectively the model can reconstruct the original input from the predicted features. By learning stronger features, the model retains more information even after passing through bottleneck layers. However, conflicts may arise between the reconstruction loss and the task-specific loss, leading to suboptimal input-output mappings. </p>
</li>
<li><p><strong>Deep supervision</strong>: In deep supervision, intermediate layers receive guidance through auxiliary loss functions, in addition to the primary supervision signal from backpropagation. Prediction layers are added between hidden layers to compute auxiliary losses, which are then combined with the main loss (e.g., cross-entropy). This method strengthens feature learning in earlier layers, facilitating better gradient flow and mitigating vanishing gradients. It supports more stable training, improves multi-task learning, and enhances generalization. However, it can lead to error accumulation, where intermediate losses propagate inaccuracies into the overall loss function. Additionally, information lost in shallower layers cannot be recovered by deeper ones.</p>
</li>
</ol>
<h2>Programmable Gradient Information</h2>
<p>To solve the previous issues, YOLOv9 utilizes <strong>Programmable Gradient Information</strong> (PGI). PGI tries to solve the issues caused by information bottlenecks and gradient inefficiency by combining both deep supervision and reversible architectures. </p>
<p>The motive of deep supervision in object detection, as I mentioned before, is to act as guidance for a model&#39;s layers to ensure they are learning the right features. A popular method of implementing deep supervision is through auxiliary branches. These are temporary branches which will act as checks for the hidden deep layers in a network by supervising them on the intermediate representation they produce. The auxiliary branch makes the intermediate layers perform predictions based on the representations they produce. This also has an added effect of breaking down the final learning objective into more manageable tasks. The final loss into a standard main loss (\(L_{main}\)) computed at the end of the network and auxiliary losses computed at the intermediate predictions in the auxiliary layers (\(L_{aux}\)). We can of course represent it as :</p>
<p>$$<br>\mathcal{L}_{total} = \mathcal{L}_{main} + \sum_{i} \lambda_i \mathcal{L}_{aux,i}<br>$$</p>
<p>Where \(\lambda_i\) is the weighing coefficient for each auxiliary loss. These auxiliary losses guide the intermediate layers to learn features more effectively, ensuring better representation across the network.</p>
<h2>PGI Architecture</h2>
<p>PGI definitely works well in concept, but adding more neurons to a model is not always the answer because it will lead to slower inference. The auxiliary layers will increase inference costs by 20% when added. However we don&#39;t have to work about that with the auxiliary layers because they are only present during the training phase to supervise intermediate layers. During inference, we &quot;turn off&quot; the auxiliary layers. Auxiliary layers also only aim to add new, important information that is missing in the intermediate representations. Hence, this concept will not underparameterize our model because we&#39;re not passing the entirety of original information from the image again, just the essential bits.</p>
<p><div class="flex justify-center my-8"><img src="/images/PGI.png" alt="alt text" title="PGI" class="rounded-lg w-1/2"></div></p>
<h3>Observations in previous architectures</h3>
<p>Authors found high performance in many models with reversible architectures. DynamicNet uses YoloV7 and merges it with CBNet architecture which has multi-level reversible branches with high parameter utilization. YoloV9 builds on DynamicNet architecture to design reversible branches on which it implements PGI.</p>
<ul>
<li><p>Deep supervision works by two methods:</p>
<ul>
<li><p>Guiding intermediate layers by introducing auxiliary losses for each intermediate layer using prediction layers between hidden layers.</p>
</li>
<li><p>Guiding feature maps to directly have properties that are present in the target image using depth or segmentation loss.</p>
</li>
</ul>
</li>
</ul>
<p>Deep supervision is usually not fit for lightweight models because it can cause underparameterization in them. This means a model does not have enough learnable parameters relative to the complexity of the model it is trying to solve. If during deep supervision the given layer in the layer hierarchy does not have enough learnable parameters to calculate auxiliary loss, it can degrade the performance of that layer, and then this degraded output is fed to the next layer which may suffer from the same problem. Hence a cascading effect starts, leading to negative performance. However PGI can reprogram semantic information and allows lightweight models to benefit from this (how exactly semantic information helps, honestly idk - authors just shove that word in without an explanation on the why for this part).</p>
<h1>The Error Accumulation Problem (and Solving it)</h1>
<p>Error accumulation in neural networks occurs when errors from intermediate layers propagate and compound as they flow through the network - causing all sorts of issues degraded performance, unstable training and is a major reason for poor generalization and convergence.</p>
<p>To understand error accumulation, let&#39;s consider how a neural network functions: each layer transforms its input, producing an output that becomes the input for the next layer. The final predictions rely heavily on the quality of these intermediate transformations. If early layers make slight errors in feature extraction, these errors are passed down the network, often amplified by subsequent layers. Over time, these compounding inaccuracies distort the final prediction.</p>
<p>For example, in an object detection task:</p>
<ul>
<li><p>Suppose the early convolutional layers fail to accurately identify edges and textures due to slight misrepresentations.</p>
</li>
<li><p>As these incomplete features move through the network, the bounding box prediction layers receive noisy or irrelevant feature maps.</p>
</li>
<li><p>The final predictions might place bounding boxes inaccurately or fail to recognize objects altogether.</p>
</li>
</ul>
<p>If \( h_i \) represents the output of the \( i \)-th layer, and \(\ \epsilon_i \) the error at this layer. The output at the next layer can be written as:<br>$$<br>h_{i+1} = f(h_i) + \epsilon_i,<br>$$<br>where \( f(\cdot) \) is the transformation function. When \( \epsilon_i \) is propagated to the next layer, it may interact non-linearly with \( f(\cdot) \), compounding the error:</p>
<p>$$<br>\epsilon_{i+1} = g(f(h_i + \epsilon_i)) - g(f(h_i)),<br>$$</p>
<p>where \( g(\cdot) \) represents the next transformation. As this process repeats across layers, \(\epsilon_{total}\) grows, particularly in deeper networks, leading to distorted representations and predictions.</p>
<p>The effect of this problem does not stop here. Since it effects our error, it effects our gradient. And because of this backpropagation amplifies the original effect of error accumulation across the network - causing gradient degredation.</p>
<p>For instance, in a deep network, if the gradient of the loss function with respect to a weight in an earlier layer is denoted by:</p>
<p>$$<br>\frac{\partial \mathcal{L}}{\partial W_i} = \frac{\partial \mathcal{L}}{\partial h_N} \cdot \frac{\partial h_N}{\partial h_{N-1}} \cdot \ldots \cdot \frac{\partial h_{i+1}}{\partial W_i},<br>$$</p>
<p>where \( N \) is the number of layers, any instability (e.g., large or small derivatives) in intermediate layers amplifies inaccuracies, affecting weight updates. This leads to the phenomenon where earlier layers either &quot;freeze&quot; (due to negligible updates) or oscillate (due to erratic updates), compromising their ability to extract meaningful features.</p>
<h2>Error Accumulation through Auxiliary Layers</h2>
<p>This problem is more than simply relevant for YOLOv9 because of its usage of deep supervision through auxiliary layers. Deep supervision, while a strategy to address such issues, can itself contribute to error accumulation if not implemented carefully. The aim of this method, as we discussed previously, is to guide intermediate layers to learn more meaningful representations. However, if the auxiliary losses are poorly designed or conflict with the primary task, they can introduce new errors. This leads to the model prioritizing learning features which do not align with the main task at all, achieving the opposing effect of what we originally wanted. </p>
<h2>Solving Error Accumulation</h2>
<p>Preventing error accumulation ideally is very straightforward - reduce the error wherever possible. As we discussed previously in the information bottleneck section, a major reason of poor predictions is information loss during data transformation between layers and learning features. Although the deep supervision (auxiliary) branch helps guide the main branch towards learning the right features, we still suffer from information loss in the auxiliary branch, the error from which gets added to the final error. </p>
<p>Another important area where information loss occurs is where the actual gradients from intermediate predictions (which are supposed to act as guidance) are not successfully mapped by the main branch to the target object. To ensure this mapping of the auxiliary gradient information happens correctly, YOLOv9 contains a multi-level auxiliary information branch. This branch acts as an intermediate between the auxiliary branch and the main branch. It aggregates the auxiliary gradients and has the specific task to integrate these gradients into the main gradient flow by making predictions of its own.</p>
<h1>Reversible Functions : Why do we need them?</h1>
<p>Reversible functions in YOLOv9 are primarily applied to enhance memory efficiency during training while maintaining model accuracy. They address the challenge of storing intermediate activations required for backpropagation. However the biggest advantage of reversible functions is the minimal information loss while reconstructing previous activations.</p>
<p>To understand reversible functions, let&#39;s consider a function \(r_\psi(X)\), which may have an inverse transformation \(v_\zeta()\). This means when we apply \(v_\zeta()\) on \(r_\psi(X)\), we get \(X\) back:</p>
<p>$$<br>    X = v_\zeta(r_\psi(X))<br>$$</p>
<p>where \(\psi\) and \(\zeta\) are parameters.</p>
<p>A reversible function results in a perfect recreation of the initial data \(X\), this means it has no information loss:</p>
<p>$$<br>    I(X,X) = I(X, r_\psi(X)) = I(X, v_\zeta(r_\psi(X))<br>$$</p>
<p>Hence the activations can be recomputed through reversible functions. This leads to better performance. This can be mathematically represented as :</p>
<p>$$<br>    X^{l+1} = X^l + f\theta ^{l+1}(X^l)<br>$$</p>
<p>This exact method was used in the PreAct ResNet model, where the equation above depicts the \(l\) th layer and a transformation function \(f\) is applied on the \(l\)-th layer. We can see that it is a reversible function as \(X^{l+1}\) can be obtained by explicitly passing \(X^l\) (Data from \(l\)-th layer) to the subsequent layers. This leads to good convergence but high complexity. Hence why PreAct ResNet must need high amount of layers to function well (susceptible to underparameterization).</p>
<p>We can pose the information bottleneck equation above as a mapping from input \(X\) to target \(Y\):</p>
<p>$$<br>    I(X,Y) \ge I(Y,X) \ge I(Y, f\theta(X) \ge \dots \ge I(Y, \hat Y)<br>$$ </p>
<p>Because of underparameterization in the shallow layers, a lot of information can be lost in the first few layers itself during \(I(Y,X)\). If we lose information in the start, the succeeding transformation functions will have no way to recover the lost information. Hence the goal for getting reliable gradients is minimizing information loss while mapping \(X\) to \(Y\) as in \(I(Y,X)\) from \(I(X,X)\).</p>
<p>To mitigate the problems caused by solely utilizing reversible functions without making the model extremely beefy, we simply selectively utilize the reversible function property in combination with the auxiliary branch during training. These layers are placed in shallow layers where the bottleneck is more prominent, as well as in the auxiliary branch itself. </p>
<p>For example, consider an intermediate activation \( X^{l} \) at layer \( l \). When transformed by a reversible function \( r_\psi \), it produces \( X^{l+1} = X^l + f_\theta(X^l) \), where \( f_\theta \) is the transformation applied. To reconstruct \( X^l \) during backpropagation, we compute:</p>
<p>$$<br>X^l = X^{l+1} - f_\theta(X^l),<br>$$</p>
<p>ensuring that the original data is retrievable. This framework avoids the explicit storage of \( X^l \), significantly reducing memory usage. However, this process becomes computationally expensive for all layers due to the repetitive evaluations of \( f_\theta \). </p>
<p>This is where we try to see where exactly we can cut corners just enough while reconstructing our activations - by asking if it is necessary to reconstruct all activations at all? YOLOv9 does this by using an approximation function to reconstruct only the activations which are relevant and contribute importantly to the gradient flow. We can dynamically apply a mask \(M\) on the activations which we want to recompute. Then, we define the approximation function \( v_\zeta \) which reconstructs the masked input efficiently:</p>
<p>$$<br>X = v_\zeta(r_\psi(X) \cdot M),<br>$$</p>
<p>where \( M \) ensures that only the relevant activations contributing to the gradient flow are computed. This selective reconstruction reduces the computational load while maintaining the integrity of the gradients.</p>
<h1>Better feature extraction with GELAN</h1>
<p>Deep models lose information during the progressive feature extraction process due to the information bottleneck principle, which we argued previously. This is caused due to repeated data (feature) transformations. If we find a method to learn and propagate gradients without the use of excessive transformations, it can potentially decrease model degradation from information loss usually incurred from the transformations. Turns out there does exist such a method, called Cross Stage Partial Networks or CSPNet. </p>
<p>CSPNet is a foundational architecture built to improve gradient flow, reduce computational complexity and enhance feature extraction efficiency of deep networks. It originally was used to solve the issue of redundant gradient information and information loss in deep convolutional networks, which share a lot of similarities with YOLOv9. CSPNet does this by using a split-transform-merge architecture:</p>
<ul>
<li><p>Part 1 is sent through a series of transformations (e.g., convolutions, activation functions).</p>
</li>
<li><p>Part 2 bypasses the transformations</p>
</li>
</ul>
6:["$","$Le",null,{"children":["$","$Lf",null,{"posts":[{"slug":"rubiks-cube-1","title":"Teaching Language Models to Solve Rubik's Cubes (Part I)","date":"2025-10-23","author":"$undefined","tags":["reinforcement learning","computer science","deep learning"],"image":"","description":"How hard could it be?","content":"$10"},{"slug":"yolov9","title":"Understanding the YOLOv9 Paper","date":"2024-02-29","author":"snowclipsed","tags":["vision","research","computer science"],"image":"","description":"My notes on the YOLOv9 paper.","content":"$11"}],"initialPost":"$6:props:children:props:posts:0"}]}]
