<!DOCTYPE html><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/0e4a976e9af4d5f0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-7725581f1a14205e.js"/><script src="/_next/static/chunks/4bd1b696-ec2ee73d25d4a1c4.js" async=""></script><script src="/_next/static/chunks/517-e97375825a119be1.js" async=""></script><script src="/_next/static/chunks/main-app-d5ed8eea04898169.js" async=""></script><script src="/_next/static/chunks/app/layout-0c4d063dd6fe438d.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/not-found-4509230feeaeaa83.js" async=""></script><script src="/_next/static/chunks/321-14e9b0818460c0f7.js" async=""></script><script src="/_next/static/chunks/738-cb387f4241b1872b.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-727f3d52741632b8.js" async=""></script><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" content="#000000"/><title>Understanding the YOLOv9 Paper</title><meta name="description" content="My notes on the YOLOv9 paper."/><link rel="manifest" href="/site.webmanifest"/><link rel="icon" href="/favicon.ico" sizes="any"/><link rel="icon" href="/favicon-16x16.png" sizes="16x16" type="image/png"/><link rel="icon" href="/favicon-32x32.png" sizes="32x32" type="image/png"/><link rel="icon" href="/favicon-192x192.png" sizes="192x192" type="image/png"/><link rel="icon" href="/favicon-512x512.png" sizes="512x512" type="image/png"/><link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000"/><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&amp;display=swap" rel="stylesheet"/><script>
              // Prevent flash of wrong theme
              const savedTheme = localStorage.getItem('theme') || 'dark';
              document.documentElement.classList.add(savedTheme);

              window.MathJax = {
                tex: {
                  inlineMath: [['\\(', '\\)']],
                  displayMath: [['$$', '$$']],
                  processEscapes: true,
                },
                options: {
                  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
              };
            </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased transition-colors duration-100 dark:bg-black dark:text-white bg-white text-black"><script src="/_next/static/chunks/webpack-7725581f1a14205e.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[8308,[\"177\",\"static/chunks/app/layout-0c4d063dd6fe438d.js\"],\"ThemeProvider\"]\n3:I[5244,[],\"\"]\n4:I[3866,[],\"\"]\n5:I[8173,[\"560\",\"static/chunks/app/blog/%5Bslug%5D/not-found-4509230feeaeaa83.js\"],\"\"]\n7:I[6213,[],\"OutletBoundary\"]\n9:I[6213,[],\"MetadataBoundary\"]\nb:I[6213,[],\"ViewportBoundary\"]\nd:I[4835,[],\"\"]\n:HL[\"/_next/static/css/0e4a976e9af4d5f0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"DseyFdEXswZN2CKs8hI_F\",\"p\":\"\",\"c\":[\"\",\"blog\",\"yolov9\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"yolov9\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/0e4a976e9af4d5f0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"dark\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.googleapis.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.gstatic.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"href\":\"https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700\u0026display=swap\",\"rel\":\"stylesheet\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#000000\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              // Prevent flash of wrong theme\\n              const savedTheme = localStorage.getItem('theme') || 'dark';\\n              document.documentElement.classList.add(savedTheme);\\n\\n              window.MathJax = {\\n                tex: {\\n                  inlineMath: [['\\\\\\\\(', '\\\\\\\\)']],\\n                  displayMath: [['$$', '$$']],\\n                  processEscapes: true,\\n                },\\n                options: {\\n                  skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']\\n                }\\n              };\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"antialiased transition-colors duration-100 dark:bg-black dark:text-white bg-white text-black\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen flex justify-center items-center\",\"children\":[\"$\",\"div\",null,{\"style\":{\"transform\":\"scale(0.60)\",\"transformOrigin\":\"center top\",\"width\":\"100%\"},\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]}]]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"yolov9\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[\"$\",\"div\",null,{\"className\":\"min-h-screen flex items-center justify-center\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center space-y-4\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold\",\"children\":\"404\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl\",\"children\":\"Post Not Found\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-500\",\"children\":\"The blog post you're looking for doesn't exist.\"}],[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"text-blue-500 hover:underline\",\"children\":\"Return Home\"}]]}]}]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",null,[\"$\",\"$L7\",null,{\"children\":\"$L8\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"spQ9AjbDqbya-DF6X7eZN\",{\"children\":[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],null]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Understanding the YOLOv9 Paper\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"My notes on the YOLOv9 paper.\"}],[\"$\",\"link\",\"3\",{\"rel\":\"manifest\",\"href\":\"/site.webmanifest\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"sizes\":\"any\"}],[\"$\",\"link\",\"5\",{\"rel\":\"icon\",\"href\":\"/favicon-16x16.png\",\"sizes\":\"16x16\",\"type\":\"image/png\"}],[\"$\",\"link\",\"6\",{\"rel\":\"icon\",\"href\":\"/favicon-32x32.png\",\"sizes\":\"32x32\",\"type\":\"image/png\"}],[\"$\",\"link\",\"7\",{\"rel\":\"icon\",\"href\":\"/favicon-192x192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"}],[\"$\",\"link\",\"8\",{\"rel\":\"icon\",\"href\":\"/favicon-512x512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],[\"$\",\"link\",\"9\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-touch-icon.png\",\"sizes\":\"180x180\"}],[\"$\",\"link\",\"10\",{\"rel\":\"mask-icon\",\"href\":\"/safari-pinned-tab.svg\",\"color\":\"#000000\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"e:I[8881,[\"321\",\"static/chunks/321-14e9b0818460c0f7.js\",\"738\",\"static/chunks/738-cb387f4241b1872b.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-727f3d52741632b8.js\"],\"default\"]\nf:I[7738,[\"321\",\"static/chunks/321-14e9b0818460c0f7.js\",\"738\",\"static/chunks/738-cb387f4241b1872b.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-727f3d52741632b8.js\"],\"default\"]\n10:T4f65,"])</script><script>self.__next_f.push([1,"\u003cp\u003eThis is another paper in the YOLO iteration. I like vision papers a lot and these are my notes from my reading of the paper.\u003c/p\u003e\n\u003cp\u003eOriginal paper \u003ca href=\"https://arxiv.org/abs/2402.13616\"\u003elink\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003eWhat exactly is the problem with current models?\u003c/h1\u003e\n\u003cp\u003e Object detection models can be simplified as learning the most important features in a training set which can allow it to predict the location (bounding box) or segmentation map of a given test object. Ideally, we want such models to be robustly generalized - an object detection world model which can one shot any test object in any scenario as long as it has seen objects of a similar class. There has been a push towards achieving this ideal but not without roadblocks in convergence, which the paper focuses a lot on. \u003c/p\u003e\n\u003cp\u003e The authors argue that in part a lot of poor/slow convergence is caused by the model not being able to strike the perfect balance between compression (learning the least amount of important features to get the job done) and relevance (which features are actually the most relevant?). This is essentially what the information bottleneck problem is.\u003c/p\u003e\n\u003ch1\u003eWhat\u0026#39;s an information bottleneck?\u003c/h1\u003e\n\u003cp\u003eAn information bottleneck is when we compress high dimensional input data into lower dimension features by retaining the most important features to reduce computational complexity for training and inference, but at the same time it leads to data loss because we have to trim some features. \u003c/p\u003e\n\u003cp\u003eThis is what almost every modern deep learning method does, and is essentially its goal. Deep learning is an inherent compression optimization problem. A model tries to learn the least amount of most important features by a priority basis - that is how we get feature maps and attention. The best model is the one which generalizes perfectly over a concept so as to require learn the least number of features possible while retaining enough information to achieve the best results.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s hard to measure the ideal amount of information loss from information bottlenecks. The authors in the paper argue that the loss from information bottlenecks may still be non-negligible in many modern techniques because it is simply learning the wrong features and hence the wrong mapping between input and predictions.\u003c/p\u003e\n\u003cp\u003e\u003cdiv class=\"flex justify-center my-8\"\u003e\u003cimg src=\"/images/infobottleneck1.png\" alt=\"alt text\" title=\"Information Bottleneck\" class=\"rounded-lg w-1/2\"\u003e\u003c/div\u003e\u003c/p\u003e\n\u003cp\u003eThe information loss due to the bottleneck can be described by in terms of mutual information:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e    I(X,X) \\ge I(X, f\\theta(X) \\ge I(X, g\\phi(f\\theta(X)))\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003eHere \\(I\\) is the mutual information. Mathematically, \\(MI(X;Y)\\) is defined  as:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003eMI(X;Y) = âˆ‘p(x,y) \\log_2(\\frac{p(x,y)}{p(x)p(y)})\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003e..where \\(p(x)\\) and \\(p(y)\\) are the marginal probability distributions of X and \\(Y\\), respectively, and \\(p(x,y)\\) is their joint probability distribution. Intuitively, MI measures how much knowing the value of X reduces uncertainty about Y, or conversely, how much knowing the value of Y reduces uncertainty about X. \\(f\\) and \\(g\\) are transformation functions with trainable parameters \\(\\theta\\) and \\(\\phi\\).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat does this even mean?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cdiv class=\"flex justify-center my-8\"\u003e\u003cimg src=\"/images/infobottleneck2.png\" alt=\"alt text\" title=\"Information Bottleneck\" class=\"rounded-lg w-1/2\"\u003e\u003c/div\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eThis represents that as more neural transformations are applied, the more information is lost. As in, deeper layers mean more information loss since there\u0026#39;s consecutive application of transformation functions (neurons).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThis means a model with deeper layers retain lesser info about both the input and target.\u003cbr\u003eHence it would naturally perform worse.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eA model with larger number of parameters has much more parameters and can learn larger number of features (information) about the data.\u003cbr\u003eThis is why width is important in deep networks than depth itself.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThis increase in width can only increase the scope of learning more information by simply increasing the number of params, but the information loss per param is still the same (or, often increased because of more connections).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003eHow do we get rid of this data loss?\u003c/h1\u003e\n\u003cp\u003eThe paper identifies 3 ways of dealing with data loss from information bottleneck.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eReversibility\u003c/strong\u003e: Reversibility is a method where we can compute/reconstruct the activations of a hidden/intermediate layer \\(Y_N\\) from a subsequent layer \\(Y_{N+1}\\) during backpropagation. By eliminating the need to store intermediate activations, this approach significantly enhances memory efficiency, leading to more compact networks. However, reversible architectures often require additional layers, increasing computational complexity as a trade-off for reduced memory usage.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMasked modeling\u003c/strong\u003e: Masked modeling improves feature extraction by training the model to predict missing (masked) features in the input data, using a reconstruction loss. This loss measures how effectively the model can reconstruct the original input from the predicted features. By learning stronger features, the model retains more information even after passing through bottleneck layers. However, conflicts may arise between the reconstruction loss and the task-specific loss, leading to suboptimal input-output mappings. \u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eDeep supervision\u003c/strong\u003e: In deep supervision, intermediate layers receive guidance through auxiliary loss functions, in addition to the primary supervision signal from backpropagation. Prediction layers are added between hidden layers to compute auxiliary losses, which are then combined with the main loss (e.g., cross-entropy). This method strengthens feature learning in earlier layers, facilitating better gradient flow and mitigating vanishing gradients. It supports more stable training, improves multi-task learning, and enhances generalization. However, it can lead to error accumulation, where intermediate losses propagate inaccuracies into the overall loss function. Additionally, information lost in shallower layers cannot be recovered by deeper ones.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eProgrammable Gradient Information\u003c/h2\u003e\n\u003cp\u003eTo solve the previous issues, YOLOv9 utilizes \u003cstrong\u003eProgrammable Gradient Information\u003c/strong\u003e (PGI). PGI tries to solve the issues caused by information bottlenecks and gradient inefficiency by combining both deep supervision and reversible architectures. \u003c/p\u003e\n\u003cp\u003eThe motive of deep supervision in object detection, as I mentioned before, is to act as guidance for a model\u0026#39;s layers to ensure they are learning the right features. A popular method of implementing deep supervision is through auxiliary branches. These are temporary branches which will act as checks for the hidden deep layers in a network by supervising them on the intermediate representation they produce. The auxiliary branch makes the intermediate layers perform predictions based on the representations they produce. This also has an added effect of breaking down the final learning objective into more manageable tasks. The final loss into a standard main loss (\\(L_{main}\\)) computed at the end of the network and auxiliary losses computed at the intermediate predictions in the auxiliary layers (\\(L_{aux}\\)). We can of course represent it as :\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\\mathcal{L}_{total} = \\mathcal{L}_{main} + \\sum_{i} \\lambda_i \\mathcal{L}_{aux,i}\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003eWhere \\(\\lambda_i\\) is the weighing coefficient for each auxiliary loss. These auxiliary losses guide the intermediate layers to learn features more effectively, ensuring better representation across the network.\u003c/p\u003e\n\u003ch2\u003ePGI Architecture\u003c/h2\u003e\n\u003cp\u003ePGI definitely works well in concept, but adding more neurons to a model is not always the answer because it will lead to slower inference. The auxiliary layers will increase inference costs by 20% when added. However we don\u0026#39;t have to work about that with the auxiliary layers because they are only present during the training phase to supervise intermediate layers. During inference, we \u0026quot;turn off\u0026quot; the auxiliary layers. Auxiliary layers also only aim to add new, important information that is missing in the intermediate representations. Hence, this concept will not underparameterize our model because we\u0026#39;re not passing the entirety of original information from the image again, just the essential bits.\u003c/p\u003e\n\u003cp\u003e\u003cdiv class=\"flex justify-center my-8\"\u003e\u003cimg src=\"/images/PGI.png\" alt=\"alt text\" title=\"PGI\" class=\"rounded-lg w-1/2\"\u003e\u003c/div\u003e\u003c/p\u003e\n\u003ch3\u003eObservations in previous architectures\u003c/h3\u003e\n\u003cp\u003eAuthors found high performance in many models with reversible architectures. DynamicNet uses YoloV7 and merges it with CBNet architecture which has multi-level reversible branches with high parameter utilization. YoloV9 builds on DynamicNet architecture to design reversible branches on which it implements PGI.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eDeep supervision works by two methods:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eGuiding intermediate layers by introducing auxiliary losses for each intermediate layer using prediction layers between hidden layers.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eGuiding feature maps to directly have properties that are present in the target image using depth or segmentation loss.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDeep supervision is usually not fit for lightweight models because it can cause underparameterization in them. This means a model does not have enough learnable parameters relative to the complexity of the model it is trying to solve. If during deep supervision the given layer in the layer hierarchy does not have enough learnable parameters to calculate auxiliary loss, it can degrade the performance of that layer, and then this degraded output is fed to the next layer which may suffer from the same problem. Hence a cascading effect starts, leading to negative performance. However PGI can reprogram semantic information and allows lightweight models to benefit from this (how exactly semantic information helps, honestly idk - authors just shove that word in without an explanation on the why for this part).\u003c/p\u003e\n\u003ch1\u003eThe Error Accumulation Problem (and Solving it)\u003c/h1\u003e\n\u003cp\u003eError accumulation in neural networks occurs when errors from intermediate layers propagate and compound as they flow through the network - causing all sorts of issues degraded performance, unstable training and is a major reason for poor generalization and convergence.\u003c/p\u003e\n\u003cp\u003eTo understand error accumulation, let\u0026#39;s consider how a neural network functions: each layer transforms its input, producing an output that becomes the input for the next layer. The final predictions rely heavily on the quality of these intermediate transformations. If early layers make slight errors in feature extraction, these errors are passed down the network, often amplified by subsequent layers. Over time, these compounding inaccuracies distort the final prediction.\u003c/p\u003e\n\u003cp\u003eFor example, in an object detection task:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eSuppose the early convolutional layers fail to accurately identify edges and textures due to slight misrepresentations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eAs these incomplete features move through the network, the bounding box prediction layers receive noisy or irrelevant feature maps.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe final predictions might place bounding boxes inaccurately or fail to recognize objects altogether.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf \\( h_i \\) represents the output of the \\( i \\)-th layer, and \\(\\ \\epsilon_i \\) the error at this layer. The output at the next layer can be written as:\u003cbr\u003e$$\u003cbr\u003eh_{i+1} = f(h_i) + \\epsilon_i,\u003cbr\u003e$$\u003cbr\u003ewhere \\( f(\\cdot) \\) is the transformation function. When \\( \\epsilon_i \\) is propagated to the next layer, it may interact non-linearly with \\( f(\\cdot) \\), compounding the error:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\\epsilon_{i+1} = g(f(h_i + \\epsilon_i)) - g(f(h_i)),\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003ewhere \\( g(\\cdot) \\) represents the next transformation. As this process repeats across layers, \\(\\epsilon_{total}\\) grows, particularly in deeper networks, leading to distorted representations and predictions.\u003c/p\u003e\n\u003cp\u003eThe effect of this problem does not stop here. Since it effects our error, it effects our gradient. And because of this backpropagation amplifies the original effect of error accumulation across the network - causing gradient degredation.\u003c/p\u003e\n\u003cp\u003eFor instance, in a deep network, if the gradient of the loss function with respect to a weight in an earlier layer is denoted by:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\\frac{\\partial \\mathcal{L}}{\\partial W_i} = \\frac{\\partial \\mathcal{L}}{\\partial h_N} \\cdot \\frac{\\partial h_N}{\\partial h_{N-1}} \\cdot \\ldots \\cdot \\frac{\\partial h_{i+1}}{\\partial W_i},\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003ewhere \\( N \\) is the number of layers, any instability (e.g., large or small derivatives) in intermediate layers amplifies inaccuracies, affecting weight updates. This leads to the phenomenon where earlier layers either \u0026quot;freeze\u0026quot; (due to negligible updates) or oscillate (due to erratic updates), compromising their ability to extract meaningful features.\u003c/p\u003e\n\u003ch2\u003eError Accumulation through Auxiliary Layers\u003c/h2\u003e\n\u003cp\u003eThis problem is more than simply relevant for YOLOv9 because of its usage of deep supervision through auxiliary layers. Deep supervision, while a strategy to address such issues, can itself contribute to error accumulation if not implemented carefully. The aim of this method, as we discussed previously, is to guide intermediate layers to learn more meaningful representations. However, if the auxiliary losses are poorly designed or conflict with the primary task, they can introduce new errors. This leads to the model prioritizing learning features which do not align with the main task at all, achieving the opposing effect of what we originally wanted. \u003c/p\u003e\n\u003ch2\u003eSolving Error Accumulation\u003c/h2\u003e\n\u003cp\u003ePreventing error accumulation ideally is very straightforward - reduce the error wherever possible. As we discussed previously in the information bottleneck section, a major reason of poor predictions is information loss during data transformation between layers and learning features. Although the deep supervision (auxiliary) branch helps guide the main branch towards learning the right features, we still suffer from information loss in the auxiliary branch, the error from which gets added to the final error. \u003c/p\u003e\n\u003cp\u003eAnother important area where information loss occurs is where the actual gradients from intermediate predictions (which are supposed to act as guidance) are not successfully mapped by the main branch to the target object. To ensure this mapping of the auxiliary gradient information happens correctly, YOLOv9 contains a multi-level auxiliary information branch. This branch acts as an intermediate between the auxiliary branch and the main branch. It aggregates the auxiliary gradients and has the specific task to integrate these gradients into the main gradient flow by making predictions of its own.\u003c/p\u003e\n\u003ch1\u003eReversible Functions : Why do we need them?\u003c/h1\u003e\n\u003cp\u003eReversible functions in YOLOv9 are primarily applied to enhance memory efficiency during training while maintaining model accuracy. They address the challenge of storing intermediate activations required for backpropagation. However the biggest advantage of reversible functions is the minimal information loss while reconstructing previous activations.\u003c/p\u003e\n\u003cp\u003eTo understand reversible functions, let\u0026#39;s consider a function \\(r_\\psi(X)\\), which may have an inverse transformation \\(v_\\zeta()\\). This means when we apply \\(v_\\zeta()\\) on \\(r_\\psi(X)\\), we get \\(X\\) back:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e    X = v_\\zeta(r_\\psi(X))\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003ewhere \\(\\psi\\) and \\(\\zeta\\) are parameters.\u003c/p\u003e\n\u003cp\u003eA reversible function results in a perfect recreation of the initial data \\(X\\), this means it has no information loss:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e    I(X,X) = I(X, r_\\psi(X)) = I(X, v_\\zeta(r_\\psi(X))\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003eHence the activations can be recomputed through reversible functions. This leads to better performance. This can be mathematically represented as :\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e    X^{l+1} = X^l + f\\theta ^{l+1}(X^l)\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003eThis exact method was used in the PreAct ResNet model, where the equation above depicts the \\(l\\) th layer and a transformation function \\(f\\) is applied on the \\(l\\)-th layer. We can see that it is a reversible function as \\(X^{l+1}\\) can be obtained by explicitly passing \\(X^l\\) (Data from \\(l\\)-th layer) to the subsequent layers. This leads to good convergence but high complexity. Hence why PreAct ResNet must need high amount of layers to function well (susceptible to underparameterization).\u003c/p\u003e\n\u003cp\u003eWe can pose the information bottleneck equation above as a mapping from input \\(X\\) to target \\(Y\\):\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e    I(X,Y) \\ge I(Y,X) \\ge I(Y, f\\theta(X) \\ge \\dots \\ge I(Y, \\hat Y)\u003cbr\u003e$$ \u003c/p\u003e\n\u003cp\u003eBecause of underparameterization in the shallow layers, a lot of information can be lost in the first few layers itself during \\(I(Y,X)\\). If we lose information in the start, the succeeding transformation functions will have no way to recover the lost information. Hence the goal for getting reliable gradients is minimizing information loss while mapping \\(X\\) to \\(Y\\) as in \\(I(Y,X)\\) from \\(I(X,X)\\).\u003c/p\u003e\n\u003cp\u003eTo mitigate the problems caused by solely utilizing reversible functions without making the model extremely beefy, we simply selectively utilize the reversible function property in combination with the auxiliary branch during training. These layers are placed in shallow layers where the bottleneck is more prominent, as well as in the auxiliary branch itself. \u003c/p\u003e\n\u003cp\u003eFor example, consider an intermediate activation \\( X^{l} \\) at layer \\( l \\). When transformed by a reversible function \\( r_\\psi \\), it produces \\( X^{l+1} = X^l + f_\\theta(X^l) \\), where \\( f_\\theta \\) is the transformation applied. To reconstruct \\( X^l \\) during backpropagation, we compute:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003eX^l = X^{l+1} - f_\\theta(X^l),\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003eensuring that the original data is retrievable. This framework avoids the explicit storage of \\( X^l \\), significantly reducing memory usage. However, this process becomes computationally expensive for all layers due to the repetitive evaluations of \\( f_\\theta \\). \u003c/p\u003e\n\u003cp\u003eThis is where we try to see where exactly we can cut corners just enough while reconstructing our activations - by asking if it is necessary to reconstruct all activations at all? YOLOv9 does this by using an approximation function to reconstruct only the activations which are relevant and contribute importantly to the gradient flow. We can dynamically apply a mask \\(M\\) on the activations which we want to recompute. Then, we define the approximation function \\( v_\\zeta \\) which reconstructs the masked input efficiently:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003eX = v_\\zeta(r_\\psi(X) \\cdot M),\u003cbr\u003e$$\u003c/p\u003e\n\u003cp\u003ewhere \\( M \\) ensures that only the relevant activations contributing to the gradient flow are computed. This selective reconstruction reduces the computational load while maintaining the integrity of the gradients.\u003c/p\u003e\n\u003ch1\u003eBetter feature extraction with GELAN\u003c/h1\u003e\n\u003cp\u003eDeep models lose information during the progressive feature extraction process due to the information bottleneck principle, which we argued previously. This is caused due to repeated data (feature) transformations. If we find a method to learn and propagate gradients without the use of excessive transformations, it can potentially decrease model degradation from information loss usually incurred from the transformations. Turns out there does exist such a method, called Cross Stage Partial Networks or CSPNet. \u003c/p\u003e\n\u003cp\u003eCSPNet is a foundational architecture built to improve gradient flow, reduce computational complexity and enhance feature extraction efficiency of deep networks. It originally was used to solve the issue of redundant gradient information and information loss in deep convolutional networks, which share a lot of similarities with YOLOv9. CSPNet does this by using a split-transform-merge architecture:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003ePart 1 is sent through a series of transformations (e.g., convolutions, activation functions).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ePart 2 bypasses the transformations\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"])</script><script>self.__next_f.push([1,"11:T8887,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eWhy this project?\u003c/h1\u003e\n\u003cp\u003eA satellite tracker is something I have wanted to make since a long time. I had researched about \u003ca href=\"https://en.wikipedia.org/wiki/Two-line_element_set\"\u003etwo line elements\u003c/a\u003e and how one can generate ground tracks for satellites using complex algorithms like SGP4, like in \u003ca href=\"https://github.com/anoved/Ground-Track-Generator\"\u003ethis repo\u003c/a\u003e. I wanted to understand concepts of orbital mechanics and map projection through a practical project\u003c/p\u003e\n\u003cp\u003eTo deploy my project, please go to \u003ca href=\"https://github.com/snowclipsed/SatTrack\"\u003ethis repository\u003c/a\u003e and follow the README instructions!\u003c/p\u003e\n\u003ch1\u003eApproach\u003c/h1\u003e\n\u003cp\u003eA satellite tracker in principle works by displaying the coordinates of a given satellite at a point in time either through projecting its overhead coordinates on a map (2D) or displaying the satellite\u0026#39;s position in a 3D environment. For this project, I use the classic first approach to project the location of a satellite on a map. \u003c/p\u003e\n\u003cp\u003eTo do this, we must look at the entire system through a Earth-centered reference frame in 3 dimensions. Then we can understand what exactly a projection is in the context of orbiting bodies. A projection is the intersecting point of the ground with a line drawn from the Earth\u0026#39;s center to the satellite, such that the line is normal to the spherical coordinate system/the ground. \u003cd-cite key=\"snyder1981map\"\u003e\u003c/d-cite\u003e\u003c/p\u003e\n\u003cp\u003eThe crux of the problem is that the Earth and the satellite are moving at different speeds, and the reference frame is not. Both these bodies are also moving at non-constant velocities (since most satellites do not have a perfectly eccentric orbit, their speeds are not the same throughout the orbit). \u003cd-cite key=\"stackoverflow_post\"\u003e\u003c/d-cite\u003e \u003cd-cite key=\"space_stackexchange_post\"\u003e\u003c/d-cite\u003e \u003c/p\u003e\n\u003cp\u003eThen, we convert the projection of a satellite\u0026#39;s coordinates on the 3D sphere into 2D by changing out map projection from WGS84 to a cylindrical projection like mercerator.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003e Note : \u003c/b\u003e I will not be going into detail about orbital mechanics and map projections in this specific blog since this is more oriented towards the implementation of the project, but I will cover them in a seperate blog in the future! However, in case you are interested, check out the references in the end. \u003cd-cite key=\"neacsu_snyder\"\u003e\u003c/d-cite\u003e\u003c/p\u003e\n\u003cp\u003eLuckily, there are several GiS tools which we can use to calculate and project satellite coordinates to maps. One such popular tool is \u003ca href=\"https://www.arcgis.com/index.html\"\u003eArcGIS\u003c/a\u003e which can allow one to display interactive maps and perform geocode and display geocode based geometries on a map. ArcGIS offers a developer API key which can be used to build applications for non-consumer purposes. We will use the Java SDK and API service from ArcGIS to render our map and plot the satellite as a moving point on the map. \u003c/p\u003e\n\u003cp\u003eHowever, we still have to figure out how to fetch the position of the satellite in real time. To do this, we can either compute the current position of the satellite by propagating two line elements to the current timestamp, or fetch the coordinates directly from a satellite tracker API. Many such free APIs exist, and I chose \u003ca href=\"https://www.n2yo.com/api/\"\u003eN2YO\u0026#39;s REST API\u003c/a\u003e for the job. \u003c/p\u003e\n\u003ch1\u003eProcedure\u003c/h1\u003e\n\u003cp\u003eI developed the satellite tracker in the IntelliJ IDEA IDE. This can be done in any other Java IDE.\u003c/p\u003e\n\u003cp\u003eLet us first instantiate what functionalities we need in our tracker. To do that, let us revisit what exactly a satellite tracker means. In essence, a satellite tracker is a program which can display the live location of a given satellite. Some trackers also carry the capability to display the ground track of a satellite.\u003c/p\u003e\n\u003cp\u003eFrom this, we can extract our first goal, to display the location of one satellite on the map. But first, we first must display the map itself.\u003c/p\u003e\n\u003ch2\u003eDisplaying a map using ArcGIS\u003c/h2\u003e\n\u003cp\u003eArcGIS supports using Java to display an interactive map using a JavaFX application. JavaFX is a software platform and graphical user interface (GUI) toolkit for creating rich desktop applications. This means we can follow the standard JavaFx lifecyle of Initialization, Start, Scene, User interaction and updation, and Termination. We first initiate an Application class which has the main and the start functions. \u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003epublic class App extends Application {\n\n    public static void main(String[] args) {\n        Application.launch(args);\n    }\n\n    public void start(Stage stage) {\n    }\n}\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eJavaFX renders UI components in a heirarichal order. The start function passes a \u003ccode\u003estage\u003c/code\u003e as a component. In JavaFX, \u003ccode\u003estage\u003c/code\u003e represents an application window, and all the GUI is housed within that window. It is the first component in the heirarchy. A \u003ccode\u003estage\u003c/code\u003e usually houses a GUI container called a \u003ccode\u003escene\u003c/code\u003e. A \u003ccode\u003estage\u003c/code\u003e may possess multiple \u003ccode\u003escenes\u003c/code\u003e but only one is shown at a time. \u003c/p\u003e\n\u003cp\u003eA \u003ccode\u003escene\u003c/code\u003e houses \u003ccode\u003epane(s)\u003c/code\u003e, which are the root node to which GUI components are attached.  A \u003ccode\u003epane\u003c/code\u003e can be of many types depending on how we want our UI to render. You can read more about panes \u003ca href=\"https://docs.oracle.com/javafx/2/layout/builtin_layouts.htm\"\u003ehere\u003c/a\u003e\u003cd-cite key=\"oraclejavafxlayouts\"\u003e\u003c/d-cite\u003e. The type of pane we will be using for this project is called a StackPane. A StackPane is exactly what it sounds like, it is based on the Stack data structure and the nodes which are added to the StackPane first are rendered last, in the LIFO order. All of this comes together in 3 lines of code.\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eStackPane stackPane = new StackPane();\n        Scene scene = new Scene(stackPane);\n        stage.setScene(scene);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eNext, we can start adding the map UI nodes. ArcGIS provides an SDK to render a map using their free developer API. You can import the libraries from ArcGIS by following \u003ca href=\"https://developers.arcgis.com/java/install-and-set-up/\"\u003ethese instructions\u003c/a\u003e to use Gradle/Maven. I use Gradle in my setup in the repository. \u003c/p\u003e\n\u003cp\u003eTo access the API key we must first store it as a string in a seperate file which we can read the API string from.\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003e String yourAPIKey = System.getProperty(\"apiKey\");\n ArcGISRuntimeEnvironment.setApiKey(yourAPIKey);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eThen, we can create a new ArcGIS map and set the map to a MapView GUI node. \u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eArcGISMap map = new ArcGISMap(BasemapStyle.ARCGIS_IMAGERY_STANDARD);\n\n      // create a map view and set the map to it\n      mapView = new MapView();\n      mapView.setMap(map);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eThis will allow us to display the map in a JavaFX application window.\u003c/p\u003e\n\u003cp\u003eNext, we will learn how to fetch the required location information for the satellite using our RESTful API.\u003c/p\u003e\n\u003ch2\u003eFetching Satellite Position\u003c/h2\u003e\n\u003cp\u003eTo fetch the position we are making use of \u003ca href=\"https://www.n2yo.com/api/\"\u003eN2YO\u0026#39;s free satellite API\u003c/a\u003e. To accomplish this, we need to define what variables we need from the API. Then, we can make a GET request to the API which then outputs a response in JSON containing the information we need.\u003c/p\u003e\n\u003cp\u003eThe base URL format that we will be using is :\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003ejson\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-json\"\u003e/positions/\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003eid\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e/\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003eobserver_lat\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e/\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003eobserver_lng\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e/\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003eobserver_alt\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e/\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003eseconds\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eLet us define a class \u003ccode\u003eSatellite\u003c/code\u003e which we will use to perform the GET request and store the information from the request. We will pass the variables to pass into the GET request as parameters to this class.\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eSatellite(String satID, String sec, String obsLat, String obsLong, String APIKEY) throws URISyntaxException, IOException, InterruptedException {\n    ...\n\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eNext, we will use HttpRequest to build a GET request to send to N2YO.\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eHttpRequest getRequest = HttpRequest.newBuilder()\n                .uri(new URI(\"https://api.n2yo.com/rest/v1/satellite/positions/\" + satID + \"/\" + obsLat + \"/\" + obsLong + \"/0/\" + sec + \"/\u0026amp;apiKey=\"+ APIKEY))\n                .GET()\n                .build();\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eWe will then create an HttpClient and make a GET request to N2YO and try to get a response from the API. \u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eHttpClient httpClient = HttpClient.newHttpClient();\nHttpResponse\u0026lt;String\u003e getResponse = httpClient.send(getRequest, HttpResponse.BodyHandlers.ofString());\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eSince this response is being recieved as a Json String, we will have to parse this using a Json String parser to utilize the components inside. To do this, we first must look at a sample response given in N2YO\u0026#39;s website:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003e{\n  \"info\": {\n    \"satname\": \"SPACE STATION\",\n    \"satid\": 25544,\n    \"transactionscount\": 5\n  },\n  \"positions\": [\n    {\n      \"satlatitude\": -39.90318514,\n      \"satlongitude\": 158.28897924,\n      \"sataltitude\": 417.85,\n      \"azimuth\": 254.31,\n      \"elevation\": -69.09,\n      \"ra\": 44.77078138,\n      \"dec\": -43.99279118,\n      \"timestamp\": 1521354418\n    },\n    {\n      \"satlatitude\": -39.86493451,\n      \"satlongitude\": 158.35261287,\n      \"sataltitude\": 417.84,\n      \"azimuth\": 254.33,\n      \"elevation\": -69.06,\n      \"ra\": 44.81676119,\n      \"dec\": -43.98086419,\n      \"timestamp\": 1521354419\n    }\n  ]\n}\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eWe note that the response is in a nested format, so we must use define two classes for the \u003ccode\u003einfo\u003c/code\u003e and \u003ccode\u003epositions\u003c/code\u003e section. Each of these classes will contain the get and set function for all the variables. On top of that, we must define a \u003ccode\u003eRequest\u003c/code\u003e class which encapsulates both the subclasses. The names of the variables should be the same as the ones in the Json response.\u003c/p\u003e\n\u003cp\u003eOnce we have created those classes, we can then use ObjectMapper to map the response body to the given variables inside the classes. We can then access these variables for the stored information.\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eObjectMapper mapper = new ObjectMapper();\n\nRequest request = mapper.readValue(getResponse.body(), Request.class);\n\n\nthis.satname = request.info.satname;\nthis.satlong = request.positions.get(0).satlongitude;\nthis.satlat = request.positions.get(0).satlatitude;\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eNow that we have the information required, all that\u0026#39;s left is to display the satellite as a point on the map and update the location in real time!\u003c/p\u003e\n\u003ch2\u003eDisplaying the satellite on the map\u003c/h2\u003e\n\u003cp\u003eBefore we start displaying the satellite on the map, we need to create a map. In a from-scratch implementation we would consider adding a map and then calculating the coordinates per pixel on the map ourselves. This is a hard problem. Luckily, ARCGiS does this entire process for us. All we need to do is create a JavaFX application window, and then declare a MapView variable. This is our infinite scrollable and pannable workspace inside the JavaFX Application window which will house the map (so that we can zoom in and out and do everything we can with modern mapping applications like Google Maps).\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003epublic class App extends Application {\n\n    private MapView mapView;\n    ...\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eNext, we create and call the map itself:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eArcGISMap map = new ArcGISMap(BasemapStyle.ARCGIS_STREETS_NIGHT);\nmapView.setMap(map);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eThere\u0026#39;s a lot of map styles and can be accessed through BasemapStyle. I chose one with night mode cause it\u0026#39;s cool.\u003c/p\u003e\n\u003cp\u003eNext, we create a graphics overlay to display our graphics (like our satellite image or any other line or point):\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eGraphicsOverlay graphicsOverlay = new GraphicsOverlay();\ngraphicsOverlay.setLabelsEnabled(true);\nmapView.getGraphicsOverlays().add(graphicsOverlay);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eWe then create a point to dislay on the overlay:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eFloat[] coords = satAPICall(satID);\nPoint point = new Point(coords[0], coords[1], SpatialReferences.getWgs84());\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eThen we set a PictureMarker symbol to the point\u0026#39;s coordinates:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003ePictureMarkerSymbol satmarker = new PictureMarkerSymbol(\"src/Images/satimage.png\");\nsatmarker.setHeight(40);\nsatmarker.setWidth(40);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eFinally, so that we can apply the image on top of the overlay so it\u0026#39;s rendered, we must create a JavaFX Graphic object, and add our graphic to the overlay:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eGraphic satpoint = new Graphic(point, satmarker);\ngraphicsOverlay.getGraphics().add(satpoint);\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eThis will display our satellite, but right now we do not have any access to the coordinates we just found through the API. We will then define a function to set our point to a location by calling the API:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003epublic void setpoint(Graphic point, Graphic text, MapView mapView) throws URISyntaxException, IOException, InterruptedException {\n    Float[] newcoords = satAPICall(satID);\n    Point newpoint = new Point(newcoords[0], newcoords[1], SpatialReferences.getWgs84());\n    point.setGeometry(newpoint);\n    mapView.setViewpoint(new Viewpoint(newcoords[1], newcoords[0], scale));\n}\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eHere, satAPICall is just an API calling function. You might notice that this function is not being called enough, so we will add a thread which keeps updating every N seconds. We set N as 10 at the top of our file.\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003eThread updateThread = new Thread(() -\u003e {\n    while (true) {\n        try {\n            setpoint(satpoint, sattextgraphic, mapView);\n            countdown(timeremaining);\n        } catch (URISyntaxException | IOException | InterruptedException e) {\n            throw new RuntimeException(e);\n        }\n        try {\n            Thread.sleep(frequency);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n});\n\nupdateThread.start();\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eNice! We can now display the satellite image. However, we have no way to actually choose any satellite right now. So let\u0026#39;s try defining a list of satellites and a drop down (combo box) menu the queries for user input on that list:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003e\nString[] satList = {\n    \"ISS\",\n    \"Hubble Space Telescope\",\n    \"IRIDIUM 167\",\n    \"STARLINK-30783\"\n};\n\nComboBox\u0026lt;String\u003e combo_box = new ComboBox\u0026lt;\u003e(FXCollections.observableArrayList(satList));\ncombo_box.setValue(\"ISS\"); // Default Value\n\nstackPane.getChildren().add(combo_box);\nStackPane.setAlignment(combo_box, Pos.TOP_RIGHT);\nStackPane.setMargin(combo_box, new Insets(125, 100, 0, 0));\n\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eWe will add a corresponding listener for the combo box:\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003ecombo_box.setOnAction((event) -\u003e {\n    String selection = combo_box.getValue();\n    satID = returnNORADID(selection);\n    try {\n        setpoint(satpoint, sattextgraphic, mapView);\n        countdown(timeremaining);\n    } catch (URISyntaxException | IOException | InterruptedException e) {\n        throw new RuntimeException(e);\n    }\n    sattext.setText(selection);\n});\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  \u003cp\u003eNow, we will implement a very simple switch statement based function to return the NORAD ID of the satellites based on what satellite name the user has selected which we can use to make an API call with :\u003c/p\u003e\n\n    \u003cdiv class=\"relative my-4 group\"\u003e\n      \u003cdiv class=\"absolute -inset-px bg-gradient-to-r from-transparent via-white/20 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-200\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"relative\"\u003e\n        \u003cdiv class=\"flex items-center justify-between px-4 py-2 bg-black/10 dark:bg-white/10 border-b border-black/20 dark:border-white/20\"\u003e\n          \u003cspan class=\"text-sm opacity-60\"\u003eplaintext\u003c/span\u003e\n          \u003cbutton \n            class=\"px-2 py-1 text-xs border border-transparent hover:border-current opacity-60 hover:opacity-100 transition-all duration-200\"\n            onclick=\"navigator.clipboard.writeText(this.parentElement.nextElementSibling.textContent)\"\n          \u003e\n            Copy\n          \u003c/button\u003e\n        \u003c/div\u003e\n        \u003cpre class=\"p-4 bg-black/5 dark:bg-white/5 overflow-x-auto\"\u003e\n          \u003ccode class=\"language-plaintext\"\u003epublic Integer returnNORADID(String satOption) {\n    int ID;\n    switch (satOption) {\n        case \"ISS\":\n            ID = 25544;\n            break;\n        case \"IRIDIUM 167\":\n            ID = 43931;\n            break;\n        case \"STARLINK-30783\":\n            ID = 58130;\n            break;\n        case \"Hubble Space Telescope\":\n            ID = 20580;\n            break;\n        default:\n            ID = 25544;\n    }\n    return ID;\n}\u003c/code\u003e\n        \u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  "])</script><script>self.__next_f.push([1,"6:[\"$\",\"$Le\",null,{\"children\":[\"$\",\"$Lf\",null,{\"posts\":[{\"slug\":\"yolov9\",\"title\":\"Understanding the YOLOv9 Paper\",\"date\":\"2024-02-29\",\"author\":\"snowclipsed\",\"tags\":[\"vision\",\"research\",\"computer science\"],\"image\":\"\",\"description\":\"My notes on the YOLOv9 paper.\",\"content\":\"$10\"},{\"slug\":\"sattrack\",\"title\":\"Satellite Tracker\",\"date\":\"2023-11-31\",\"author\":\"$undefined\",\"tags\":[\"orbital mechanics\",\"java\",\"computer science\"],\"image\":\"\",\"description\":\"A real- time satellite tracker written in Java.\",\"content\":\"$11\"}],\"initialPost\":\"$6:props:children:props:posts:0\"}]}]\n"])</script></body></html>